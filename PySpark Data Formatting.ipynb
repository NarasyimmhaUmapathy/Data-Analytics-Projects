{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in functions needed aka Indexer, Vectorising functions, scaling etc\n",
    "\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.ml.feature import StringIndexer\n",
    "from pyspark.ml.feature import MinMaxScaler\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read in data\n",
    "\n",
    "path = \"Datasets/autism-screening-for-toddlders/\"\n",
    "df = spark.read.csv(path+\"Toddler Autism dataset July 2018.csv\",inferSchema=True,header=True)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.limit(6).toPandas()\n",
    "df.printSchema()  #important to check data types and what\n",
    "df.groupBy(\"Class/ASD Traits\").count().show(100) #check for class balance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_columns = df.columns[1:-1] #take input columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dependent_var = \"Class/ASD Traits\"\n",
    "\n",
    "renamed = df.withColumn(\"label_str\",df[dependent_var].cast(StringType())\n",
    "indexer = StringIndexer(inputCol=\"label_str\",outputCol=\"label\")\n",
    "indexed = indexer.fit(renamed).transform(renamed)\n",
    "                                                           \n",
    "                                           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_inputs = []\n",
    "string_inputs = []\n",
    "for column in input_columns:\n",
    "    if str(indexed.schema[column].dataType) == \"StringType\":\n",
    "       indexer = StringIndexer(inputCol=column,outputCol=column+\"_num\")\n",
    "       indexed = indexer.fit(indexed).transform(indexed)\n",
    "       new_col_name = column + \"_num\"\n",
    "       string_inputs.append(new_col_name)\n",
    "    else:\n",
    "       numeric_inputs.append(column)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = {}\n",
    "for col in numeric_inputs:\n",
    "    d[col] = indexed.approxQuantile(col,[0.01,0.99],0.25 )\n",
    "\n",
    "for col in numeric_inputs:\n",
    "    skew = indexed.agg(skewness(indexed[col])).collect()\n",
    "    skew = skew[0][0]\n",
    "    if skew > 1:\n",
    "        indexed = indexed.withColumn(col,\\\n",
    "                  log(when(df[col] < d[col][0],d[col][0]\\\n",
    "                  .when(indexed[col] > d[col][1],dcol[1]\\\n",
    "                  .otherwise(indexed(col)+1).alias(col)\n",
    "                  print(col,\"has been treated for pos skew\",skew)\n",
    "    elif skew < -1:\n",
    "                        \n",
    "        indexed = indexed.withColumn(col,\\\n",
    "                  exp(when(df[col] < d[col][0],d[col][0]\\\n",
    "                  .when(indexed[col] > d[col][1],dcol[1]\\\n",
    "                  .otherwise(indexed(col)).alias(col)\n",
    "                  print(col,\"has been treated for neg skew\",skew)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_list = numeric_inputs + string_inputs #combining a single features list which has been indexed\n",
    "\n",
    "assembler = VectorAssembler(inputCols=features_list,outputCol=\"features\")\n",
    "output = assembler.transform(indexed).select(\"features\",\"labels\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dealing with negatives, but this scaler doesnt deal well with outliers! use robust scaler\n",
    "\n",
    "scaler = MinMaxScaler(inputCols=\"features\",outputCol=\"scaled_features\",min=0,max=1000)\n",
    "scalerModel = scaler.fit(output)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
